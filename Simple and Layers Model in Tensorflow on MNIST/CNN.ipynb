{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification\n",
    "This is a project in which I have implemented an Convolutional Neural Network both using tensorflow Low Level and High Level API with the required Preprocessing Steps and on one of the most recognized MNIST Handwritten datasets.\n",
    "\n",
    "Both the model perform well enough on both the training dataset and test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "Here we preprocess the data that we need to train the model on and hence represent it in a way such that it can be used for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct \n",
    "import numpy as np\n",
    "\n",
    "## Here we define a function that loads the mnist dataset and also normalize the images pixel values between -1 and 1 as it generarlizes better this way for the model\n",
    "def load_mnist(path,kind = 'train'):\n",
    "    labels_path = os.path.join(path,'%s-labels.idx1-ubyte' % kind)\n",
    "    \n",
    "    images_path = os.path.join(path, '%s-images.idx3-ubyte' % kind)\n",
    "    \n",
    "    with open(labels_path,'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "        \n",
    "        labels = np.fromfile(lbpath,dtype = np.uint8)\n",
    "        \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\",imgpath.read(16))\n",
    "        \n",
    "        images = np.fromfile(imgpath,dtype = np.uint8).reshape(len(labels),784)\n",
    "        \n",
    "        images = ((images/255.) - .5)*2\n",
    "        \n",
    "    return images, labels\n",
    "\n",
    "X_data , y_data = load_mnist('./mnist/',kind = 'train')    ## Loading the training and test data respectively\n",
    "X_test , y_test = load_mnist('./mnist/',kind = 't10k')\n",
    "X_train , y_train = X_data[:50000] , y_data[:50000]        ## Dividing the training dataset into training and validation sets respectively\n",
    "X_valid , y_valid = X_data[50000:] , y_data[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we define a helper function that we can use to divide the \n",
    "## dataset into respective mini-batches for training the model\n",
    "def batch_generator(X,y,batch_size = 64,shuffle = False,random_seed = None):\n",
    "    idx = np.arange(y.shape[0])\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        rng.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "        \n",
    "    for i in range(0,X.shape[0],batch_size):\n",
    "        yield (X[i:i+batch_size,:], y[i:i+batch_size])\n",
    "## Hence it yields an iterator for this location in the dataset and hence we can train on this partiular set and hence it also doesn't affect the storage that much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we standardize the dataset so that it is efficient for the model to train on the dataset\n",
    "mean_vals = np.mean(X_train,axis = 0)  \n",
    "std_val = np.std(X_train)             ## we don't use standard deviation respective row-wise because it could be possible that the pixels could be constant over a whole row or column and hencce which could lead to dividing by zero\n",
    "X_train_centered = (X_train - mean_vals)/std_val    ## standardizing the dataset respectively\n",
    "X_valid_centered = (X_valid - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "## Hence now we have preprocessed the dataset and further move on to implementing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low level API\n",
    "Here we implement the Low level CNN by defining all the built in functions from scratch to understand the intricacies of the model and hence we train the model and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "## Here we define the Convolution layers which would be used initially for extracting important features from the images\n",
    "\n",
    "def conv_layer(input_tensor,name,kernel_size,n_output_channels,padding_mode = 'SAME',strides = (1,1,1,1)):\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = input_tensor.get_shape().as_list()\n",
    "        n_input_channels = input_shape[-1]\n",
    "        \n",
    "        weights_shape = list(kernel_size) + [n_input_channels,n_output_channels]\n",
    "        weights = tf.get_variable(name = '_weigths',shape = weights_shape)\n",
    "        biases = tf.get_variable(name = '_biases',initializer=tf.zeros(shape = [n_output_channels]))\n",
    "        conv = tf.nn.conv2d(input = input_tensor,filter = weights,strides = strides,padding = padding_mode)\n",
    "        conv = tf.nn.bias_add(conv,biases,name = 'net_pre-activation')\n",
    "        conv = tf.nn.relu(conv,name = 'activation')\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we define the fully connnected layer that wouuld be trained after some specified number of convolutional layers and hence would be responsible for training on important extracted features\n",
    "def fc_layer(input_tensor,name,n_output_units,activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = input_tensor.get_shape().as_list()[1:]\n",
    "        n_input_units = np.prod(input_shape)\n",
    "        if len(input_shape) > 1:\n",
    "            input_tensor = tf.reshape(input_tensor,shape = (-1,n_input_units))\n",
    "        \n",
    "        weights_shape = [n_input_units,n_output_units]\n",
    "        weights = tf.get_variable(name = '_weights',shape = weights_shape)\n",
    "        biases = tf.get_variable(name = '_biases',initializer = tf.zeros(shape = [n_output_units]))\n",
    "        layer = tf.matmul(input_tensor,weights)\n",
    "        layer = tf.nn.bias_add(layer,biases,name = 'net_pre-activation')\n",
    "\n",
    "        if activation_fn is None:\n",
    "            return layer\n",
    "        layer = activation_fn(layer , name = 'activation')\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this function we use the helper functions defined above and Build the whole CNN Model from Scratch\n",
    "def build_cnn():\n",
    "    tf_x = tf.placeholder(tf.float32,shape = [None,784],name = 'tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32,shape = [None],name = 'tf_y')\n",
    "    tf_x_image = tf.reshape(tf_x,shape = [-1,28,28,1],name = 'tf_x_reshaped')\n",
    "    tf_y_onehot = tf.one_hot(indices = tf_y,depth = 10,dtype = tf.float32,name = 'tf_onehot')\n",
    "    \n",
    "    print('\\nBuilding 1st layer:')\n",
    "    ## It has alternate layers of convolution and subsampling\n",
    "    h1 = conv_layer(tf_x_image,name='conv_1',kernel_size = (5,5),padding_mode = 'VALID',n_output_channels = 32)\n",
    "    \n",
    "    h1_pool = tf.nn.max_pool(h1,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n",
    "    \n",
    "    print('\\nBuilding 2nd layer:')\n",
    "    h2 = conv_layer(h1_pool,name = 'conv_2',kernel_size = (5,5),padding_mode='VALID',n_output_channels=64)\n",
    "    \n",
    "    h2_pool = tf.nn.max_pool(h2,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n",
    "    \n",
    "    print('\\nBuilding 3rd layer:')\n",
    "    ## After two layers of Convolution we add the Fully Connected Layers\n",
    "    h3 = fc_layer(h2_pool,name = 'fc_3',n_output_units=1024,activation_fn=tf.nn.relu)\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32,name = 'fc_keep_prob')\n",
    "    ## Here we also add the Dropout Layer as a regularizer so that the model doesn't overfit\n",
    "    h3_drop = tf.nn.dropout(h3,keep_prob = keep_prob,name = 'dropout_layer')\n",
    "    \n",
    "    print('\\nBuilding 4th layer:')\n",
    "    ## Here we build the last layer of Fully connected layer after which we give out the logits\n",
    "    h4 = fc_layer(h3_drop,name = 'fc_4',n_output_units=10,activation_fn=None)\n",
    "    ## Here we define the softmax for evaluating respective probabilities and the labels for the images\n",
    "    predictions = {'probabilities':tf.nn.softmax(h4,name = 'probabilities'),'labels':tf.cast(tf.argmax(h4,axis = 1),tf.int32,name = 'labels')}\n",
    "    ## Here we define the cross entropy loss for the classifier\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = h4,labels = tf_y_onehot),name = 'cross_entropy_loss')\n",
    "    ## Here we define the respective optimizer for the function\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,name = 'train_op')\n",
    "    ## Evaluate the predictions\n",
    "    correct_predictions = tf.equal(predictions['labels'],tf_y,name = 'correct_preds')\n",
    "    ## calculate the accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions,tf.float32),name = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we define the reespective helper functions to save the model and restore it whenever needed and use it for further dataset\n",
    "## Saving the model\n",
    "def save(saver,sess,epoch,path = './model/'):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    print('Saving model in %s' %path)\n",
    "    saver.save(sess,os.path.join(path,'cnn-model.ckpt'),global_step = epoch)\n",
    "## Loading the model    \n",
    "def load(saver,sess,path,epoch):\n",
    "    print('Loading model from %s' % path)\n",
    "    saver.restore(sess, os.path.join(path,'cnn-model.ckpt-%d' % epoch))\n",
    "## Training the model\n",
    "def train(sess,training_set,validation_set=None,initialize = True,epochs = 20,shuffle = True,dropout = 0.5,random_seed = None):\n",
    "    X_data = np.array(training_set[0])\n",
    "    y_data = np.array(training_set[1])\n",
    "    training_loss = []\n",
    "    \n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    np.random.seed(random_seed)\n",
    "    for epoch in range(1,epochs+1):\n",
    "        batch_gen = batch_generator(X_data,y_data,shuffle = shuffle)\n",
    "        avg_loss = 0.0\n",
    "        for i,(batch_x,batch_y) in enumerate(batch_gen):\n",
    "            feed = {'tf_x:0' : batch_x,'tf_y:0': batch_y,'fc_keep_prob:0':dropout}\n",
    "            loss, _ = sess.run(['cross_entropy_loss:0','train_op'],feed_dict = feed)\n",
    "            avg_loss += loss\n",
    "        training_loss.append(avg_loss/(i+1))\n",
    "        \n",
    "        print('Epoch %02d Training Avg. Loss: %7.3f ' %(epoch,avg_loss),end = ' ')\n",
    "        \n",
    "        if validation_set is not None:\n",
    "            feed = {'tf_x:0':validation_set[0],\n",
    "                    'tf_y:0':validation_set[1],\n",
    "                    'fc_keep_prob:0':1.0}\n",
    "            valid_acc = sess.run('accuracy:0',feed_dict = feed)\n",
    "            print(' Validation Acc: %7.3f' % valid_acc)\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "## prerdicting from the model\n",
    "def predict(sess,X_test,return_proba = False):\n",
    "    feed = {'tf_x:0':X_test,'fc_keep_prob:0':1.0}\n",
    "    if return_proba:\n",
    "        return sess.run('probabilities:0',feed_dict = feed)\n",
    "    else:\n",
    "        return sess.run('labels:0',feed_dict= feed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1st layer:\n",
      "\n",
      "Building 2nd layer:\n",
      "\n",
      "Building 3rd layer:\n",
      "\n",
      "Building 4th layer:\n"
     ]
    }
   ],
   "source": [
    "## Here we use some values for learning rate which turned out to be efficient \n",
    "learning_rate = 1e-4\n",
    "random_seed = 123\n",
    "## building the model\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    build_cnn()\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 Training Avg. Loss: 273.745   Validation Acc:   0.974\n",
      "Epoch 02 Training Avg. Loss:  74.354   Validation Acc:   0.984\n",
      "Epoch 03 Training Avg. Loss:  50.427   Validation Acc:   0.984\n",
      "Epoch 04 Training Avg. Loss:  39.705   Validation Acc:   0.988\n",
      "Epoch 05 Training Avg. Loss:  32.010   Validation Acc:   0.988\n",
      "Epoch 06 Training Avg. Loss:  27.765   Validation Acc:   0.988\n",
      "Epoch 07 Training Avg. Loss:  23.297   Validation Acc:   0.991\n",
      "Epoch 08 Training Avg. Loss:  20.094   Validation Acc:   0.990\n",
      "Epoch 09 Training Avg. Loss:  17.445   Validation Acc:   0.992\n",
      "Epoch 10 Training Avg. Loss:  16.106   Validation Acc:   0.991\n",
      "Epoch 11 Training Avg. Loss:  12.627   Validation Acc:   0.992\n",
      "Epoch 12 Training Avg. Loss:  11.549   Validation Acc:   0.992\n",
      "Epoch 13 Training Avg. Loss:  10.415   Validation Acc:   0.990\n",
      "Epoch 14 Training Avg. Loss:   9.284   Validation Acc:   0.992\n",
      "Epoch 15 Training Avg. Loss:   8.617   Validation Acc:   0.992\n",
      "Epoch 16 Training Avg. Loss:   7.419   Validation Acc:   0.993\n",
      "Epoch 17 Training Avg. Loss:   6.287   Validation Acc:   0.993\n",
      "Epoch 18 Training Avg. Loss:   5.850   Validation Acc:   0.993\n",
      "Epoch 19 Training Avg. Loss:   4.821   Validation Acc:   0.992\n",
      "Epoch 20 Training Avg. Loss:   4.594   Validation Acc:   0.992\n",
      "Saving model in ./model/\n"
     ]
    }
   ],
   "source": [
    "## Training the model and checking its validation accuracy\n",
    "with tf.Session(graph = g) as sess:\n",
    "    train(sess,training_set = (X_train_centered,y_train),validation_set=(X_valid_centered,y_valid),initialize=True,random_seed=123)\n",
    "    save(saver,sess,epoch = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1st layer:\n",
      "\n",
      "Building 2nd layer:\n",
      "\n",
      "Building 3rd layer:\n",
      "\n",
      "Building 4th layer:\n",
      "Loading model from ./model/\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn-model.ckpt-20\n",
      "Test Accuracy: 99.250%\n"
     ]
    }
   ],
   "source": [
    "del g\n",
    "## checking if the load and save method works and then predicting from the model\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    build_cnn()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session(graph = g2) as sess:\n",
    "    load(saver,sess,epoch = 20,path = './model/')\n",
    "    \n",
    "    preds = predict(sess,X_test_centered,return_proba=False)\n",
    "    print('Test Accuracy: %.3f%%' % (100*np.sum(preds == y_test)/len(y_test)))\n",
    "## Hence we obtain a good enough testing accuracy which is equivalent to the validation accurarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model/\n",
      "INFO:tensorflow:Restoring parameters from ./model/cnn-model.ckpt-20\n",
      "Epoch 01 Training Avg. Loss:   4.677   Validation Acc:   0.992\n",
      "Epoch 02 Training Avg. Loss:   4.158   Validation Acc:   0.992\n",
      "Epoch 03 Training Avg. Loss:   3.544   Validation Acc:   0.992\n",
      "Epoch 04 Training Avg. Loss:   3.178   Validation Acc:   0.992\n",
      "Epoch 05 Training Avg. Loss:   3.171   Validation Acc:   0.992\n",
      "Epoch 06 Training Avg. Loss:   2.638   Validation Acc:   0.991\n",
      "Epoch 07 Training Avg. Loss:   3.575   Validation Acc:   0.993\n",
      "Epoch 08 Training Avg. Loss:   2.499   Validation Acc:   0.992\n",
      "Epoch 09 Training Avg. Loss:   2.484   Validation Acc:   0.992\n",
      "Epoch 10 Training Avg. Loss:   1.835   Validation Acc:   0.993\n",
      "Epoch 11 Training Avg. Loss:   2.733   Validation Acc:   0.993\n",
      "Epoch 12 Training Avg. Loss:   1.897   Validation Acc:   0.992\n",
      "Epoch 13 Training Avg. Loss:   2.218   Validation Acc:   0.991\n",
      "Epoch 14 Training Avg. Loss:   2.267   Validation Acc:   0.993\n",
      "Epoch 15 Training Avg. Loss:   1.488   Validation Acc:   0.992\n",
      "Epoch 16 Training Avg. Loss:   1.598   Validation Acc:   0.993\n",
      "Epoch 17 Training Avg. Loss:   2.049   Validation Acc:   0.992\n",
      "Epoch 18 Training Avg. Loss:   1.800   Validation Acc:   0.993\n",
      "Epoch 19 Training Avg. Loss:   1.327   Validation Acc:   0.993\n",
      "Epoch 20 Training Avg. Loss:   1.971   Validation Acc:   0.993\n",
      "Saving model in ./model/\n",
      "Test Accuracy: 99.290%\n"
     ]
    }
   ],
   "source": [
    "## retraining the model for another 20 epochs from the loaded checkpoint and then again testing the model\n",
    "with tf.Session(graph = g2) as sess:\n",
    "    load(saver,sess,epoch = 20,path = './model/')\n",
    "    train(sess,training_set=(X_train_centered,y_train),validation_set=(X_valid_centered,y_valid),initialize = False,epochs = 20,random_seed = 123)\n",
    "    save(saver,sess,epoch = 40,path = './model/')\n",
    "    preds = predict(sess,X_test_centered,return_proba=False)\n",
    "    print('Test Accuracy: %.3f%%' %(100*np.sum(preds == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    High Level API\n",
    "Here we build the same model but using the High Level Layers API of Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "## Here we store the model in tflayers-model \n",
    "## We define the class ConvNN which would contain all the required methods for training and building the model\n",
    "class ConvNN(object):\n",
    "    def __init__(self,batchsize = 64,epochs = 20,learning_rate = 1e-4,dropout_rate = 0.5,shuffle = True,random_seed = None):\n",
    "        np.random.seed(random_seed)\n",
    "        self.batchsize = batchsize\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            tf.set_random_seed(random_seed)\n",
    "            self.build()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            self.saver = tf.train.Saver()\n",
    "        self.sess = tf.Session(graph = g)\n",
    "        \n",
    "    def build(self):\n",
    "        tf_x = tf.placeholder(tf.float32,shape = [None,784],name = 'tf_x')\n",
    "        tf_y  =tf.placeholder(tf.int32,shape = [None],name = 'tf_y')\n",
    "        \n",
    "        is_train = tf.placeholder(tf.bool,shape = (),name = 'is_train')\n",
    "        \n",
    "        tf_x_image = tf.reshape(tf_x,shape = [-1,28,28,1],name = 'input_x_2dimages')\n",
    "        tf_y_onehot = tf.one_hot(indices = tf_y,depth = 10,dtype = tf.float32,name = 'input_y_onehot')\n",
    "        \n",
    "        h1 = tf.layers.conv2d(tf_x_image,kernel_size = (5,5),filters = 32,activation = tf.nn.relu)\n",
    "        h1_pool = tf.layers.max_pooling2d(h1,pool_size=(2,2),strides=(2,2))\n",
    "        \n",
    "        h2 = tf.layers.conv2d(h1_pool,kernel_size=(5,5),filters=64,activation=tf.nn.relu)\n",
    "        h2_pool = tf.layers.max_pooling2d(h2,pool_size=(2,2),strides=(2,2))\n",
    "        \n",
    "        input_shape = h2_pool.get_shape().as_list()\n",
    "        n_input_units = np.prod(input_shape[1:])\n",
    "        h2_pool_flat = tf.reshape(h2_pool,shape = [-1,n_input_units])\n",
    "        h3 = tf.layers.dense(h2_pool_flat,1024,activation=tf.nn.relu)\n",
    "        \n",
    "        h3_drop = tf.layers.dropout(h3,rate = self.dropout_rate,training = is_train)\n",
    "        h4 = tf.layers.dense(h3_drop,10,activation = None)\n",
    "        \n",
    "        predictions = {'probabilities':tf.nn.softmax(h4,name = 'probabilities'),'labels':tf.cast(tf.argmax(h4,axis = 1),tf.int32,name ='labels')}\n",
    "        cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = h4,labels = tf_y_onehot),name = 'cross_entropy_loss')\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        optimizer = optimizer.minimize(cross_entropy_loss,name = 'train_op')\n",
    "        \n",
    "        correct_predictions = tf.equal(predictions['labels'],tf_y,name = 'correct_preds')\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions,tf.float32),name = 'accuracy')\n",
    "        \n",
    "    def save(self,epoch,path = './tflayers-model/'):\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "        print('Saving model in %s ' % path)\n",
    "        self.saver.save(self.sess,os.path.join(path,'model.ckpt'),global_step = epoch)\n",
    "    \n",
    "    def load(self,epoch,path):\n",
    "        print('Loading model from %s' % path)\n",
    "        self.saver.restore(self.sess,os.path.join(path,'model.ckpt-%d' % epoch))\n",
    "        \n",
    "    def train(self,training_set,validation_set = None,initialize = True):\n",
    "        if initialize:\n",
    "            self.sess.run(self.init_op)\n",
    "        \n",
    "        self.train_cost_ = []\n",
    "        X_data = np.array(training_set[0])\n",
    "        y_data =np.array(training_set[1])\n",
    "        \n",
    "        for epoch in range(1,self.epochs+1):\n",
    "            batch_gen = batch_generator(X_data,y_data,shuffle = self.shuffle)\n",
    "            avg_loss = 0.0\n",
    "            for i,(batch_x,batch_y) in enumerate(batch_gen):\n",
    "                feed = {'tf_x:0':batch_x,'tf_y:0':batch_y,'is_train:0':True}\n",
    "                loss, _ = self.sess.run(['cross_entropy_loss:0','train_op'],feed_dict = feed)\n",
    "                avg_loss += loss\n",
    "\n",
    "            print('Epoch %02d: Training Avg. Loss: %7.3f ' %(epoch,avg_loss),end = ' ')\n",
    "            if validation_set is not None:\n",
    "                feed = {'tf_x:0':validation_set[0],'tf_y:0':validation_set[1],'is_train:0':False}\n",
    "                valid_acc = self.sess.run('accuracy:0',feed_dict = feed)\n",
    "                print('Validation Acc: %7.3f ' % valid_acc)\n",
    "            else:\n",
    "                print()\n",
    "\n",
    "    def predict(self,X_test,return_proba = False):\n",
    "        feed = {'tf_x:0':X_test,'is_train:0':False}\n",
    "        if return_proba:\n",
    "            return self.sess.run('probabilities:0',feed_dict = feed)\n",
    "        else:\n",
    "            return self.sess.run('labels:0',feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000027763C4E708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000027763C4E708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000027763C4E708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000027763C4E708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771F9F3848>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771F9F3848>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771F9F3848>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771F9F3848>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000277200592C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000277200592C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000277200592C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000277200592C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002772000C048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002772000C048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002772000C048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002772000C048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000277200592C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000277200592C8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000277200592C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000277200592C8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000002771F5A29C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000002771F5A29C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000002771F5A29C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000002771F5A29C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771F5A29C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771F5A29C8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771F5A29C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771F5A29C8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "Epoch 01: Training Avg. Loss: 271.610  Validation Acc:   0.974 \n",
      "Epoch 02: Training Avg. Loss:  75.700  Validation Acc:   0.982 \n",
      "Epoch 03: Training Avg. Loss:  51.315  Validation Acc:   0.987 \n",
      "Epoch 04: Training Avg. Loss:  39.694  Validation Acc:   0.988 \n",
      "Epoch 05: Training Avg. Loss:  31.472  Validation Acc:   0.988 \n",
      "Epoch 06: Training Avg. Loss:  27.187  Validation Acc:   0.989 \n",
      "Epoch 07: Training Avg. Loss:  23.141  Validation Acc:   0.991 \n",
      "Epoch 08: Training Avg. Loss:  19.402  Validation Acc:   0.990 \n",
      "Epoch 09: Training Avg. Loss:  16.660  Validation Acc:   0.991 \n",
      "Epoch 10: Training Avg. Loss:  15.587  Validation Acc:   0.990 \n",
      "Epoch 11: Training Avg. Loss:  12.854  Validation Acc:   0.991 \n",
      "Epoch 12: Training Avg. Loss:  11.148  Validation Acc:   0.992 \n",
      "Epoch 13: Training Avg. Loss:  10.020  Validation Acc:   0.992 \n",
      "Epoch 14: Training Avg. Loss:   8.942  Validation Acc:   0.992 \n",
      "Epoch 15: Training Avg. Loss:   8.371  Validation Acc:   0.991 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Training Avg. Loss:   7.281  Validation Acc:   0.991 \n",
      "Epoch 17: Training Avg. Loss:   5.560  Validation Acc:   0.992 \n",
      "Epoch 18: Training Avg. Loss:   6.090  Validation Acc:   0.993 \n",
      "Epoch 19: Training Avg. Loss:   4.900  Validation Acc:   0.993 \n",
      "Epoch 20: Training Avg. Loss:   4.851  Validation Acc:   0.992 \n",
      "Saving model in ./tflayers-model/ \n"
     ]
    }
   ],
   "source": [
    "## Instantiating the model and building and training it\n",
    "cnn = ConvNN(random_seed = 123)\n",
    "cnn.train(training_set = (X_train_centered,y_train),validation_set = (X_valid_centered,y_valid),initialize = True)\n",
    "cnn.save(epoch = 20)\n",
    "## The respecting warnings are occuring due to colliding version in tensorflow and gast and would be updated later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002771FBA09C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002771FBA09C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002771FBA09C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002771FBA09C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771FB96B48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771FB96B48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771FB96B48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771FB96B48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000277202F1C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000277202F1C88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000277202F1C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000277202F1C88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771FB85C08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771FB85C08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771FB85C08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002771FB85C08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771FB9DD08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771FB9DD08>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771FB9DD08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771FB9DD08>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000002771FB76CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000002771FB76CC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000002771FB76CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000002771FB76CC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771FB76CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771FB76CC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771FB76CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002771FB76CC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "Loading model from ./tflayers-model/\n",
      "INFO:tensorflow:Restoring parameters from ./tflayers-model/model.ckpt-20\n",
      "Test Accuracy: 99.40%\n"
     ]
    }
   ],
   "source": [
    "del cnn\n",
    "## loading the model again and hence using it to predict the testing dataset\n",
    "cnn2 = ConvNN(random_seed = 123)\n",
    "cnn2.load(epoch = 20,path = './tflayers-model/')\n",
    "preds = cnn2.predict(X_test_centered)\n",
    "print('Test Accuracy: %.2f%%' %(100*np.sum(y_test == preds)/len(y_test)))\n",
    "## Hence we again obtain a good enough testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
